Search.setIndex({"docnames": ["index", "ope/index", "overview/index", "policy/index"], "filenames": ["index.rst", "ope/index.rst", "overview/index.rst", "policy/index.rst"], "titles": ["Welcome to Hopes!", "Hopes: Estimators", "Overview of Hopes package", "Hopes: Policies"], "terms": {"x": 1, "invers": 1, "probabl": [1, 3], "weight": 1, "ipw": 1, "self": 1, "normal": 1, "snipw": 1, "direct": 1, "method": [1, 3], "dm": 1, "doubli": 1, "robust": 1, "dr": 1, "trajectori": 1, "wise": 1, "import": 1, "sampl": 1, "twi": 1, "per": 1, "decis": 1, "pdi": 1, "current": [1, 3], "follow": [1, 3], "ar": [1, 3], "class": [1, 3], "op": 1, "inverseprobabilityweight": 1, "base": [1, 3], "baseestim": 1, "v_": 1, "pi_e": 1, "d": 1, "frac": 1, "1": [1, 3], "n": 1, "sum_": 1, "t": 1, "p": 1, "s_t": 1, "a_t": 1, "r_t": 1, "where": [1, 3], "i": [1, 3], "offlin": [1, 3], "collect": [1, 3], "dataset": [1, 3], "defin": [1, 3], "pi_b": 1, "target": 1, "polici": 1, "behavior": 1, "reward": [1, 3], "observ": [1, 3], "time": [1, 3], "number": 1, "thi": [1, 3], "ha": 1, "gener": 1, "high": 1, "varianc": 1, "especi": 1, "small": 1, "can": [1, 3], "improv": 1, "us": [1, 2, 3], "refer": 1, "http": [1, 2], "scholarwork": 1, "umass": 1, "edu": 1, "cgi": 1, "viewcont": 1, "articl": 1, "1079": 1, "context": 1, "cs_faculty_pub": 1, "estimate_policy_valu": 1, "float": [1, 3], "valu": [1, 3], "estimate_weighted_reward": 1, "ndarrai": [1, 3], "selfnormalizedinverseprobabilityweight": 1, "paper": 1, "nip": 1, "cc": 1, "paper_fil": 1, "2015": 1, "hash": 1, "39027dfad5138c9ca0c474d71db915c3": 1, "abstract": [1, 3], "html": 1, "directmethod": 1, "q_model_cl": 1, "type": 1, "regressionbasedrewardmodel": 1, "behavior_policy_ob": 1, "behavior_policy_act": 1, "behavior_policy_reward": 1, "steps_per_episod": 1, "int": [1, 3], "q_model_typ": 1, "str": [1, 3], "random_forest": [1, 3], "q_model_param": 1, "dict": [1, 3], "none": [1, 3], "q": 1, "A": [1, 3], "s_0": 1, "model": [1, 3], "train": [1, 3], "expect": 1, "initi": 1, "state": [1, 3], "under": [1, 3], "action": [1, 3], "taken": 1, "set": [1, 3], "supervis": 1, "learn": 1, "The": [1, 3], "perform": [1, 3], "depend": [1, 3], "qualiti": 1, "check_paramet": 1, "check": 1, "paramet": [1, 3], "valid": 1, "plu": 1, "addit": 1, "fit": [1, 3], "return": [1, 3], "statist": [1, 3], "To": [1, 3], "you": [1, 3], "need": [1, 3], "subclass": [1, 3], "It": [1, 3], "should": 1, "typic": 1, "below": 1, "abc": [1, 3], "all": [1, 3], "call": 1, "befor": 1, "overridden": [1, 3], "add": 1, "have": [1, 3], "sever": 1, "assumpt": [1, 3], "must": [1, 3], "met": 1, "coverag": 1, "non": 1, "zero": [1, 3], "take": 1, "100": 1, "necessari": [1, 3], "e": 1, "both": [1, 3], "an": [1, 3], "some": [1, 3], "we": [1, 3], "enforc": 1, "here": 1, "avoid": 1, "numer": 1, "issu": 1, "posit": 1, "neg": 1, "abl": 1, "get": 1, "lower": 1, "bound": [1, 3], "specif": 1, "comput": [1, 3], "estimate_policy_value_with_confidence_interv": 1, "num_sampl": 1, "1000": 1, "significance_level": 1, "0": [1, 3], "05": 1, "confid": 1, "interv": 1, "bootstrap": 1, "input": [1, 3], "data": [1, 3], "from": [1, 2, 3], "exampl": [1, 3], "set_paramet": 1, "target_policy_action_prob": 1, "behavior_policy_action_prob": 1, "metric": 1, "print": 1, "output": [1, 3], "lower_bound": 1, "2": 1, "upper_bound": 1, "4": 1, "mean": 1, "3": [1, 2, 3], "std": 1, "signific": 1, "level": 1, "dictionari": 1, "contain": 1, "kei": 1, "given": [1, 3], "upper": 1, "standard": 1, "deviat": 1, "receiv": 1, "support": [2, 3], "python": 2, "version": 2, "10": [2, 3], "pypi": 2, "pip": 2, "sourc": 2, "develop": 2, "git": 2, "clone": 2, "github": 2, "com": 2, "airboxlab": 2, "cd": 2, "poetri": 2, "r": 2, "requir": 2, "txt": 2, "thei": 3, "aim": 3, "provid": 3, "integr": 3, "actual": 3, "which": 3, "off": 3, "evalu": 3, "classificationbasedpolici": 3, "ob": 3, "act": 3, "classification_model": 3, "logist": 3, "model_param": 3, "classif": 3, "predict": 3, "log": 3, "likelihood": 3, "In": 3, "absenc": 3, "control": 3, "pair": 3, "would": 3, "been": 3, "regress": 3, "random": 3, "forest": 3, "mlp": 3, "usag": 3, "reg_polici": 3, "train_ob": 3, "train_act": 3, "fit_stat": 3, "act_prob": 3, "compute_action_prob": 3, "new_ob": 3, "accuraci": 3, "f1": 3, "score": 3, "log_likelihood": 3, "piecewiselinearpolici": 3, "num_seg": 3, "actions_bin": 3, "list": 3, "piecewis": 3, "linear": 3, "select": 3, "segment": 3, "threshold": 3, "slope": 3, "estim": 3, "distribut": 3, "over": 3, "drawn": 3, "bm": 3, "reset": 3, "rule": 3, "instanc": 3, "outdoor": 3, "air": 3, "function": 3, "temperatur": 3, "minimum": 3, "maximum": 3, "axi": 3, "also": 3, "help": 3, "simpl": 3, "schedul": 3, "sinc": 3, "determinist": 3, "assum": 3, "assign": 3, "almost": 3, "other": 3, "being": 3, "continu": 3, "discret": 3, "space": 3, "done": 3, "bin": 3, "nearest": 3, "rmse": 3, "r\u00b2": 3, "r2": 3, "onnxmodelbasedpolici": 3, "onnx_model_path": 3, "path": 3, "obs_input": 3, "tupl": 3, "dtype": 3, "state_dim": 3, "seq_len": 3, "prev_n_act": 3, "prev_n_reward": 3, "state_input": 3, "seq_len_input": 3, "prev_actions_input": 3, "prev_rewards_input": 3, "state_output_nam": 3, "action_output_nam": 3, "action_probs_output_nam": 3, "action_log_probs_output_nam": 3, "action_dist_inputs_output_nam": 3, "exist": 3, "onnx": 3, "make": 3, "opinion": 3, "about": 3, "structur": 3, "mai": 3, "overrid": 3, "your": 3, "doe": 3, "attent": 3, "mechan": 3, "updat": 3, "each": 3, "step": 3, "option": 3, "layer": 3, "underli": 3, "pre": 3, "rai": 3, "rllib": 3, "save": 3, "algorithm": 3, "export_policy_model": 3, "transform": 3, "pass": 3, "previou": 3, "onnx_file_path": 3, "default_polici": 3, "np": 3, "float32": 3, "32": 3, "num_transform": 3, "memori": 3, "attention_dim": 3, "state_in_0": 3, "int32": 3, "prev_act": 3, "int64": 3, "reshape_5": 3, "cond_1": 3, "merg": 3, "model_2": 3, "dense_6": 3, "biasadd": 3, "rand": 3, "15": 3, "compute_reward": 3, "onli": 3, "shape": 3, "batch_siz": 3, "obs_dim": 3, "map_input": 3, "prepar": 3, "properti": 3, "output_nam": 3, "name": 3, "By": 3, "default": 3, "subset": 3, "reset_st": 3, "functionbasedpolici": 3, "policy_funct": 3, "callabl": 3, "map": 3, "randompolici": 3, "num_act": 3, "uniformli": 3, "serv": 3, "baselin": 3, "comparison": 3, "epsilon": 3, "select_act": 3, "fals": 3, "": 3, "whether": 3, "with_epsilon": 3, "greedi": 3}, "objects": {"hopes.ope.estimators": [[1, 0, 1, "", "BaseEstimator"], [1, 0, 1, "", "DirectMethod"], [1, 0, 1, "", "InverseProbabilityWeighting"], [1, 0, 1, "", "SelfNormalizedInverseProbabilityWeighting"]], "hopes.ope.estimators.BaseEstimator": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_policy_value_with_confidence_interval"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "set_parameters"]], "hopes.ope.estimators.DirectMethod": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "fit"]], "hopes.ope.estimators.InverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.policy": [[3, 0, 1, "", "ClassificationBasedPolicy"], [3, 0, 1, "", "FunctionBasedPolicy"], [3, 0, 1, "", "OnnxModelBasedPolicy"], [3, 0, 1, "", "PiecewiseLinearPolicy"], [3, 0, 1, "", "Policy"], [3, 0, 1, "", "RandomPolicy"]], "hopes.policy.ClassificationBasedPolicy": [[3, 1, 1, "", "fit"], [3, 1, 1, "", "log_likelihoods"]], "hopes.policy.FunctionBasedPolicy": [[3, 1, 1, "", "log_likelihoods"]], "hopes.policy.OnnxModelBasedPolicy": [[3, 1, 1, "", "compute_reward"], [3, 1, 1, "", "log_likelihoods"], [3, 1, 1, "", "map_inputs"], [3, 2, 1, "", "output_names"], [3, 1, 1, "", "reset_state"]], "hopes.policy.PiecewiseLinearPolicy": [[3, 1, 1, "", "fit"], [3, 1, 1, "", "log_likelihoods"]], "hopes.policy.Policy": [[3, 1, 1, "", "compute_action_probs"], [3, 3, 1, "", "epsilon"], [3, 1, 1, "", "log_likelihoods"], [3, 1, 1, "", "select_action"], [3, 1, 1, "", "with_epsilon"]], "hopes.policy.RandomPolicy": [[3, 1, 1, "", "log_likelihoods"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property", "3": "py:attribute"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"hope": [1, 2, 3], "estim": 1, "roadmap": 1, "implement": [1, 3], "document": [1, 3], "new": [1, 3], "overview": 2, "packag": 2, "what": 2, "": 2, "box": 2, "instal": 2, "polici": 3}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Hopes: Estimators": [[1, "hopes-estimators"]], "Roadmap": [[1, "roadmap"]], "Implemented estimators": [[1, "implemented-estimators"]], "Estimators documentation": [[1, "estimators-documentation"]], "Implementing a new estimator": [[1, "implementing-a-new-estimator"]], "Overview of Hopes package": [[2, "overview-of-hopes-package"]], "What\u2019s in the box?": [[2, "what-s-in-the-box"]], "Installation": [[2, "installation"]], "Hopes: Policies": [[3, "hopes-policies"]], "Implemented policies": [[3, "implemented-policies"]], "Policies documentation": [[3, "policies-documentation"]], "Implementing a new policy": [[3, "implementing-a-new-policy"]]}, "indexentries": {"baseestimator (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.BaseEstimator"]], "directmethod (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.DirectMethod"]], "inverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting"]], "selfnormalizedinverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting"]], "check_parameters() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.check_parameters"]], "check_parameters() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.check_parameters"]], "estimate_policy_value() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value"]], "estimate_policy_value_with_confidence_interval() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_policy_value_with_confidence_interval"]], "estimate_weighted_rewards() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_weighted_rewards"]], "fit() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.fit"]], "set_parameters() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.set_parameters"]], "classificationbasedpolicy (class in hopes.policy)": [[3, "hopes.policy.ClassificationBasedPolicy"]], "functionbasedpolicy (class in hopes.policy)": [[3, "hopes.policy.FunctionBasedPolicy"]], "onnxmodelbasedpolicy (class in hopes.policy)": [[3, "hopes.policy.OnnxModelBasedPolicy"]], "piecewiselinearpolicy (class in hopes.policy)": [[3, "hopes.policy.PiecewiseLinearPolicy"]], "policy (class in hopes.policy)": [[3, "hopes.policy.Policy"]], "randompolicy (class in hopes.policy)": [[3, "hopes.policy.RandomPolicy"]], "compute_action_probs() (hopes.policy.policy method)": [[3, "hopes.policy.Policy.compute_action_probs"]], "compute_reward() (hopes.policy.onnxmodelbasedpolicy method)": [[3, "hopes.policy.OnnxModelBasedPolicy.compute_reward"]], "epsilon (hopes.policy.policy attribute)": [[3, "hopes.policy.Policy.epsilon"]], "fit() (hopes.policy.classificationbasedpolicy method)": [[3, "hopes.policy.ClassificationBasedPolicy.fit"]], "fit() (hopes.policy.piecewiselinearpolicy method)": [[3, "hopes.policy.PiecewiseLinearPolicy.fit"]], "log_likelihoods() (hopes.policy.classificationbasedpolicy method)": [[3, "hopes.policy.ClassificationBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.functionbasedpolicy method)": [[3, "hopes.policy.FunctionBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.onnxmodelbasedpolicy method)": [[3, "hopes.policy.OnnxModelBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.piecewiselinearpolicy method)": [[3, "hopes.policy.PiecewiseLinearPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.policy method)": [[3, "hopes.policy.Policy.log_likelihoods"]], "log_likelihoods() (hopes.policy.randompolicy method)": [[3, "hopes.policy.RandomPolicy.log_likelihoods"]], "map_inputs() (hopes.policy.onnxmodelbasedpolicy method)": [[3, "hopes.policy.OnnxModelBasedPolicy.map_inputs"]], "output_names (hopes.policy.onnxmodelbasedpolicy property)": [[3, "hopes.policy.OnnxModelBasedPolicy.output_names"]], "reset_state() (hopes.policy.onnxmodelbasedpolicy method)": [[3, "hopes.policy.OnnxModelBasedPolicy.reset_state"]], "select_action() (hopes.policy.policy method)": [[3, "hopes.policy.Policy.select_action"]], "with_epsilon() (hopes.policy.policy method)": [[3, "hopes.policy.Policy.with_epsilon"]]}})