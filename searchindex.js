Search.setIndex({"alltitles": {"Assumptions of importance sampling and regularization": [[3, "assumptions-of-importance-sampling-and-regularization"]], "Estimators documentation": [[1, "estimators-documentation"]], "Hopes": [[0, null]], "Hopes: Estimators": [[1, null]], "Hopes: Policies": [[4, null]], "Hopes: Selection": [[2, null]], "Hopes: finding the best policy": [[3, null]], "How does importance sampling work?": [[3, "how-does-importance-sampling-work"]], "Implementation details": [[2, "implementation-details"]], "Implemented estimators": [[1, "implemented-estimators"]], "Implemented policies": [[4, "implemented-policies"]], "Implementing a new estimator": [[1, "implementing-a-new-estimator"]], "Implementing a new policy": [[4, "implementing-a-new-policy"]], "Installation": [[0, "installation"]], "Introduction": [[2, "introduction"]], "Other methods": [[3, "other-methods"]], "Policies documentation": [[4, "policies-documentation"]], "References": [[3, "references"]], "Roadmap": [[1, "roadmap"], [2, "roadmap"]], "What\u2019s in the box?": [[0, "what-s-in-the-box"]], "What\u2019s off-policy (policy) evaluation?": [[3, "what-s-off-policy-policy-evaluation"]], "Why Hopes?": [[0, "why-hopes"]]}, "docnames": ["index", "ope/index", "ops/index", "overview/index", "policy/index"], "envversion": {"sphinx": 61, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2}, "filenames": ["index.rst", "ope/index.rst", "ops/index.rst", "overview/index.rst", "policy/index.rst"], "indexentries": {"baseestimator (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.BaseEstimator", false]], "check_parameters() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.check_parameters", false]], "check_parameters() (hopes.ope.estimators.perdecisionimportancesampling method)": [[1, "hopes.ope.estimators.PerDecisionImportanceSampling.check_parameters", false]], "check_parameters() (hopes.ope.estimators.trajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.TrajectoryWiseImportanceSampling.check_parameters", false]], "classificationbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.ClassificationBasedPolicy", false]], "compute_action_probs() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.compute_action_probs", false]], "compute_reward() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.compute_reward", false]], "directmethod (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.DirectMethod", false]], "epsilon (hopes.policy.policy property)": [[4, "hopes.policy.Policy.epsilon", false]], "estimate_policy_value() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_policy_value", false]], "estimate_policy_value() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_policy_value", false]], "estimate_policy_value() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_policy_value", false]], "estimate_policy_value() (hopes.ope.estimators.perdecisionimportancesampling method)": [[1, "hopes.ope.estimators.PerDecisionImportanceSampling.estimate_policy_value", false]], "estimate_policy_value() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value", false]], "estimate_policy_value() (hopes.ope.estimators.trajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.TrajectoryWiseImportanceSampling.estimate_policy_value", false]], "estimate_weighted_rewards() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_weighted_rewards", false]], "estimate_weighted_rewards() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_weighted_rewards", false]], "estimate_weighted_rewards() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_weighted_rewards", false]], "estimate_weighted_rewards() (hopes.ope.estimators.perdecisionimportancesampling method)": [[1, "hopes.ope.estimators.PerDecisionImportanceSampling.estimate_weighted_rewards", false]], "estimate_weighted_rewards() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_weighted_rewards", false]], "estimate_weighted_rewards() (hopes.ope.estimators.trajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.TrajectoryWiseImportanceSampling.estimate_weighted_rewards", false]], "evaluate() (hopes.ope.evaluation.offpolicyevaluation method)": [[2, "hopes.ope.evaluation.OffPolicyEvaluation.evaluate", false]], "fit() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.fit", false]], "fit() (hopes.policy.classificationbasedpolicy method)": [[4, "hopes.policy.ClassificationBasedPolicy.fit", false]], "fit() (hopes.policy.piecewiselinearpolicy method)": [[4, "hopes.policy.PiecewiseLinearPolicy.fit", false]], "functionbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.FunctionBasedPolicy", false]], "inverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting", false]], "log_probabilities() (hopes.policy.classificationbasedpolicy method)": [[4, "hopes.policy.ClassificationBasedPolicy.log_probabilities", false]], "log_probabilities() (hopes.policy.functionbasedpolicy method)": [[4, "hopes.policy.FunctionBasedPolicy.log_probabilities", false]], "log_probabilities() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.log_probabilities", false]], "log_probabilities() (hopes.policy.piecewiselinearpolicy method)": [[4, "hopes.policy.PiecewiseLinearPolicy.log_probabilities", false]], "log_probabilities() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.log_probabilities", false]], "log_probabilities() (hopes.policy.randompolicy method)": [[4, "hopes.policy.RandomPolicy.log_probabilities", false]], "map_inputs() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.map_inputs", false]], "name (hopes.policy.policy property)": [[4, "hopes.policy.Policy.name", false]], "normalize() (hopes.ope.estimators.selfnormalizedperdecisionimportancesampling method)": [[1, "hopes.ope.estimators.SelfNormalizedPerDecisionImportanceSampling.normalize", false]], "normalize() (hopes.ope.estimators.selfnormalizedtrajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.SelfNormalizedTrajectoryWiseImportanceSampling.normalize", false]], "offpolicyevaluation (class in hopes.ope.evaluation)": [[2, "hopes.ope.evaluation.OffPolicyEvaluation", false]], "offpolicyselection (class in hopes.ope.selection)": [[2, "hopes.ope.selection.OffPolicySelection", false]], "onnxmodelbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.OnnxModelBasedPolicy", false]], "output_names (hopes.policy.onnxmodelbasedpolicy property)": [[4, "hopes.policy.OnnxModelBasedPolicy.output_names", false]], "perdecisionimportancesampling (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.PerDecisionImportanceSampling", false]], "piecewiselinearpolicy (class in hopes.policy)": [[4, "hopes.policy.PiecewiseLinearPolicy", false]], "policy (class in hopes.policy)": [[4, "hopes.policy.Policy", false]], "randompolicy (class in hopes.policy)": [[4, "hopes.policy.RandomPolicy", false]], "reset_state() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.reset_state", false]], "select_action() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.select_action", false]], "select_top_k() (hopes.ope.selection.offpolicyselection static method)": [[2, "hopes.ope.selection.OffPolicySelection.select_top_k", false]], "selfnormalizedinverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting", false]], "selfnormalizedperdecisionimportancesampling (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.SelfNormalizedPerDecisionImportanceSampling", false]], "selfnormalizedtrajectorywiseimportancesampling (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.SelfNormalizedTrajectoryWiseImportanceSampling", false]], "short_name() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.short_name", false]], "short_name() (hopes.ope.estimators.selfnormalizedtrajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.SelfNormalizedTrajectoryWiseImportanceSampling.short_name", false]], "short_name() (hopes.ope.estimators.trajectorywiseimportancesampling method)": [[1, "hopes.ope.estimators.TrajectoryWiseImportanceSampling.short_name", false]], "trajectorywiseimportancesampling (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.TrajectoryWiseImportanceSampling", false]], "with_epsilon() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.with_epsilon", false]], "with_name() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.with_name", false]]}, "objects": {"hopes.ope.estimators": [[1, 0, 1, "", "BaseEstimator"], [1, 0, 1, "", "DirectMethod"], [1, 0, 1, "", "InverseProbabilityWeighting"], [1, 0, 1, "", "PerDecisionImportanceSampling"], [1, 0, 1, "", "SelfNormalizedInverseProbabilityWeighting"], [1, 0, 1, "", "SelfNormalizedPerDecisionImportanceSampling"], [1, 0, 1, "", "SelfNormalizedTrajectoryWiseImportanceSampling"], [1, 0, 1, "", "TrajectoryWiseImportanceSampling"]], "hopes.ope.estimators.BaseEstimator": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "short_name"]], "hopes.ope.estimators.DirectMethod": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "fit"]], "hopes.ope.estimators.InverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.estimators.PerDecisionImportanceSampling": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.estimators.SelfNormalizedPerDecisionImportanceSampling": [[1, 1, 1, "", "normalize"]], "hopes.ope.estimators.SelfNormalizedTrajectoryWiseImportanceSampling": [[1, 1, 1, "", "normalize"], [1, 1, 1, "", "short_name"]], "hopes.ope.estimators.TrajectoryWiseImportanceSampling": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "short_name"]], "hopes.ope.evaluation": [[2, 0, 1, "", "OffPolicyEvaluation"]], "hopes.ope.evaluation.OffPolicyEvaluation": [[2, 1, 1, "", "evaluate"]], "hopes.ope.selection": [[2, 0, 1, "", "OffPolicySelection"]], "hopes.ope.selection.OffPolicySelection": [[2, 1, 1, "", "select_top_k"]], "hopes.policy": [[4, 0, 1, "", "ClassificationBasedPolicy"], [4, 0, 1, "", "FunctionBasedPolicy"], [4, 0, 1, "", "OnnxModelBasedPolicy"], [4, 0, 1, "", "PiecewiseLinearPolicy"], [4, 0, 1, "", "Policy"], [4, 0, 1, "", "RandomPolicy"]], "hopes.policy.ClassificationBasedPolicy": [[4, 1, 1, "", "fit"], [4, 1, 1, "", "log_probabilities"]], "hopes.policy.FunctionBasedPolicy": [[4, 1, 1, "", "log_probabilities"]], "hopes.policy.OnnxModelBasedPolicy": [[4, 1, 1, "", "compute_reward"], [4, 1, 1, "", "log_probabilities"], [4, 1, 1, "", "map_inputs"], [4, 2, 1, "", "output_names"], [4, 1, 1, "", "reset_state"]], "hopes.policy.PiecewiseLinearPolicy": [[4, 1, 1, "", "fit"], [4, 1, 1, "", "log_probabilities"]], "hopes.policy.Policy": [[4, 1, 1, "", "compute_action_probs"], [4, 2, 1, "", "epsilon"], [4, 1, 1, "", "log_probabilities"], [4, 2, 1, "", "name"], [4, 1, 1, "", "select_action"], [4, 1, 1, "", "with_epsilon"], [4, 1, 1, "", "with_name"]], "hopes.policy.RandomPolicy": [[4, 1, 1, "", "log_probabilities"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property"}, "terms": {"": [2, 4], "0": [1, 2, 3, 4], "00523288": 2, "00788465": 2, "03735": 1, "05": 2, "1": [1, 2, 3, 4], "10": [0, 2, 4], "1000": 2, "1079": 1, "15": 4, "1906": 1, "2000": 3, "2015": [1, 3], "2018": 3, "2019": 3, "2020": 3, "2021": 3, "2022": 3, "2128": 2, "3": [0, 3, 4], "32": 4, "39027dfad5138c9ca0c474d71db915c3": 1, "4148": 2, "490235": 2, "497324": 2, "499158": 2, "507513": 2, "510251": 2, "522907": 2, "6": 2, "6167": 2, "72408": 2, "90": 2, "A": [1, 3, 4], "As": 3, "By": [1, 4], "In": [3, 4], "It": [0, 1, 2, 3, 4], "The": [1, 2, 3, 4], "There": 3, "To": [1, 3, 4], "_": 1, "a_t": [1, 3], "ab": 1, "abbrevi": 1, "abc": 4, "abl": 3, "about": 4, "absenc": 4, "abstract": [1, 4], "access": 3, "accur": 3, "accuraci": 4, "act": [2, 4], "act_prob": 4, "action": [0, 1, 3, 4], "action_dist_inputs_output_nam": 4, "action_log_probs_output_nam": 4, "action_output_nam": 4, "action_probs_output_nam": 4, "actions_bin": 4, "actual": [3, 4], "addit": 1, "aim": 4, "air": 4, "airboxlab": 0, "algorithm": [3, 4], "all": [1, 2, 3, 4], "allow": 3, "almost": 4, "alpha": 2, "also": [1, 3, 4], "among": [0, 3], "an": [0, 2, 3, 4], "ani": 3, "anoth": 3, "approx": 3, "approxim": 3, "ar": [0, 1, 2, 3, 4], "articl": 1, "arxiv": 1, "assign": 4, "assum": [2, 4], "assumpt": 4, "attent": 4, "attention_dim": 4, "avail": 3, "averag": 3, "avoid": 3, "axi": 4, "b": 3, "bandit": 3, "barto": 3, "base": [0, 1, 2, 3, 4], "baseestim": [1, 2], "baselin": [0, 4], "batch_siz": 4, "becaus": 3, "been": 4, "behavior": [1, 2, 3], "behavior_polici": 2, "behavior_policy_act": 1, "behavior_policy_action_prob": 2, "behavior_policy_ob": 1, "behavior_policy_reward": 1, "being": 4, "below": 1, "berg": 3, "best": [0, 2], "bia": 3, "biasadd": 4, "bin": 4, "bm": 4, "bool": 2, "bootstrap": 2, "both": [3, 4], "bound": [2, 3, 4], "build": [0, 3], "c": 3, "calcul": 2, "call": [1, 3], "callabl": 4, "can": [1, 2, 3, 4], "candid": 0, "case": 3, "categori": 3, "cc": 1, "cd": 0, "cgi": 1, "character": 0, "check": 1, "check_paramet": 1, "chen": 3, "ci": 2, "ci_method": 2, "ci_significance_level": 2, "class": [1, 2, 4], "classif": 4, "classifi": 3, "classification_model": [2, 4], "classificationbasedpolici": [2, 4], "clone": 0, "collect": [1, 3, 4], "collected_act": 2, "collected_ob": 2, "com": 0, "combin": 3, "comfort": 3, "compar": 0, "comparison": 4, "comput": [1, 2, 3, 4], "compute_action_prob": 4, "compute_reward": 4, "cond_1": 4, "confid": 2, "consider": 3, "consist": 2, "contain": 2, "context": [0, 1, 3], "continu": [3, 4], "control": [0, 3, 4], "conveni": 3, "could": 3, "counterfactu": 3, "cover": 3, "coverag": 3, "creat": 2, "crucial": 3, "cs_faculty_pub": 1, "current": [1, 2, 4], "custom": 1, "d": [1, 3], "data": [0, 2, 3, 4], "dataset": [0, 1, 2, 4], "decis": [1, 3], "default": [1, 2, 4], "default_polici": 4, "defin": [1, 4], "definit": 3, "dense_6": 4, "depend": [1, 4], "deploi": 3, "design": 0, "desir": [3, 4], "determinist": [3, 4], "develop": 0, "deviat": 2, "dict": [1, 2, 4], "dictionari": 2, "differ": 3, "difficult": 3, "direct": [1, 3], "directmethod": 1, "discount": 1, "discount_factor": 1, "discret": [3, 4], "distribut": [2, 3, 4], "divid": 1, "divis": 3, "dm": [1, 3], "do": 0, "document": 2, "doe": 4, "doesn": 3, "don": 3, "done": 4, "doubli": [1, 3], "dr": [1, 3], "drawn": 4, "dtype": 4, "due": [1, 3], "dx": 3, "dynam": 3, "e": 0, "e_": 3, "each": [2, 3, 4], "easi": 0, "edu": 1, "effici": 3, "elect": 0, "elig": 3, "empir": 3, "energi": 3, "ensur": 3, "environ": 3, "epsilon": 4, "equal": 3, "especi": [1, 3], "estim": [0, 2, 3, 4], "estimate_policy_valu": 1, "estimate_policy_value_with_confidence_interv": 2, "estimate_weighted_reward": [1, 2], "evalu": [0, 2, 4], "evaluation_result": 2, "exampl": [2, 4], "exist": 4, "expect": [1, 3], "experi": 2, "export_policy_model": 4, "express": 3, "f": 3, "f1": 4, "factor": 1, "fail_fast": 2, "fals": 4, "ff": 0, "fit": [1, 2, 3, 4], "fit_stat": 4, "flexibl": 0, "float": [1, 2, 4], "float32": 4, "follow": [1, 2, 4], "forest": 4, "formula": 2, "found": 2, "frac": [1, 2, 3], "from": [0, 1, 2, 3, 4], "function": [3, 4], "functionbasedpolici": 4, "g": 3, "gamma": 1, "gamma_t": 1, "gener": [1, 3], "git": 0, "github": 0, "given": [2, 4], "goal": 3, "good": 3, "gradient": 3, "greedi": 4, "h": 0, "ha": [1, 3], "hash": 1, "hat": 2, "have": [0, 3, 4], "help": [0, 4], "here": [1, 3], "high": [1, 3], "higher": 3, "hm": 3, "hong": 3, "howev": 3, "html": 1, "http": [0, 1], "hvac": [0, 3], "hybrid": 3, "i": [0, 1, 2, 3, 4], "ie": 1, "imagin": 0, "impact": 3, "impli": 3, "import": 1, "improv": 1, "includ": 0, "increas": 3, "infer": 2, "influenti": 3, "inform": 2, "initi": [1, 2], "input": [2, 4], "instanc": [2, 3, 4], "int": [1, 2, 3, 4], "int32": 4, "int64": 4, "integr": [3, 4], "interact": 3, "interv": 2, "intrins": 3, "introduct": 3, "intuit": 3, "invers": [1, 3], "inverseprobabilityweight": [1, 2], "ip": 3, "ipw": [1, 2, 3], "issu": 3, "j": [1, 3], "jiang": 3, "jin": 3, "joachim": 3, "k": 2, "kallu": 3, "kei": 2, "know": 3, "larg": 1, "layer": 4, "le": 3, "learn": [1, 3], "length": [1, 3], "less": 3, "let": 3, "letter": 1, "level": 2, "like": [2, 3], "linear": 4, "list": [2, 4], "ll": 3, "log": [0, 4], "log_likelihood": 4, "log_prob": 4, "logist": [2, 4], "long": 3, "longer": 3, "lower": [2, 3], "lower_bound": 2, "m": 3, "made": 3, "mai": [3, 4], "make": [3, 4], "map": 4, "map_input": 4, "mathemat": 3, "mathrm": 2, "maxim": 3, "maximum": 4, "mean": [1, 2, 3], "mechan": 4, "memori": 4, "merg": 4, "method": [1, 2, 4], "metric": 2, "minimum": 4, "minmaxscal": 3, "mlp": 4, "model": [1, 3, 4], "model_2": 4, "model_param": 4, "more": [2, 3], "mu": 2, "multipl": 2, "must": [3, 4], "n": [1, 2, 3], "name": [1, 4], "ndarrai": [1, 2, 4], "nearest": 4, "necessari": 4, "need": [1, 4], "neg": 3, "new_ob": 4, "newli": 3, "nip": 1, "non": 3, "none": [1, 4], "nor": 3, "normal": [1, 2, 3], "note": [2, 3], "now": 3, "np": 4, "num_act": [2, 4], "num_sampl": 2, "num_seg": 4, "num_transform": 4, "number": [1, 2], "numer": 3, "o": 0, "ob": [2, 4], "object": 2, "obs_dim": 4, "obs_input": 4, "observ": [0, 1, 4], "obtain": 3, "occup": 3, "off": [0, 2, 4], "offer": 0, "offlin": [0, 1, 4], "offpolicyevalu": 2, "offpolicyevaluationresult": 2, "offpolicyselect": 2, "often": 3, "olici": 0, "one": [0, 2], "onli": [2, 4], "onnx": 4, "onnx_file_path": 4, "onnx_model_path": 4, "onnxmodelbasedpolici": 4, "op": [1, 2], "oper": 1, "opinion": 4, "optimis": 0, "option": [1, 4], "org": 1, "other": 4, "out": 3, "outdoor": 4, "output": [2, 4], "output_nam": 4, "over": [1, 3, 4], "overrid": 4, "overridden": [1, 4], "p": [0, 1, 3], "p1": 2, "p2": 2, "p3": 2, "packag": 0, "pair": 4, "paper": 1, "paper_fil": 1, "paramet": [1, 2, 3, 4], "particularli": 0, "pass": 4, "path": 4, "pdi": [1, 3], "per": [1, 3], "perdecisionimportancesampl": 1, "perform": [0, 1, 3, 4], "pi_b": [1, 3], "pi_e": [1, 3], "piecewis": 4, "piecewiselinearpolici": 4, "pip": 0, "plu": 1, "poetri": 0, "polici": [0, 1, 2], "policy_funct": 4, "posit": 3, "possibl": 3, "practic": 3, "pre": 4, "precup": 3, "predict": 4, "prepar": [0, 4], "preprocess": 0, "prerequisit": 3, "prev_act": 4, "prev_actions_input": 4, "prev_n_act": 4, "prev_n_reward": 4, "prev_rewards_input": 4, "previou": [3, 4], "principl": 3, "print": 2, "probabl": [1, 3, 4], "process": 0, "prod_": 1, "produc": 2, "product": 1, "project": 3, "propens": 3, "properti": 4, "provid": [0, 1, 4], "pypi": 0, "python": 0, "q": [1, 3], "q_model_cl": 1, "q_model_param": 1, "q_model_typ": 1, "qualiti": 1, "quantil": 2, "r": [0, 3], "r2": 4, "r_t": [1, 3], "rai": 4, "rand": 4, "random": [2, 4], "random_forest": [1, 2, 4], "randompolici": [2, 4], "ratio": 3, "real": 3, "reason": 3, "reduc": 1, "reduct": 1, "refer": 1, "reg_polici": 4, "regress": [3, 4], "regressionbasedrewardmodel": 1, "reinforc": 3, "reli": 3, "replac": 2, "requir": [0, 3], "resampl": 2, "rescal": 3, "reset": 4, "reset_st": 4, "reshape_5": 4, "result": [0, 2, 3], "return": [1, 2, 4], "review": 3, "rew": 2, "reward": [1, 2, 3, 4], "rl": 0, "rllib": 4, "rmse": 4, "robust": [1, 3], "rule": [0, 3, 4], "run": 2, "r\u00b2": 4, "s_0": 1, "s_t": [1, 3], "sai": 3, "same": 3, "sampl": [1, 2], "satisfi": 3, "save": 4, "schedul": 4, "scholarwork": 1, "scope": 3, "score": [3, 4], "section": 3, "see": [2, 3], "seen": 3, "segment": 4, "select": [0, 4], "select_act": 4, "select_top_k": 2, "self": [1, 3], "selfnormalizedinverseprobabilityweight": [1, 2], "selfnormalizedperdecisionimportancesampl": 1, "selfnormalizedtrajectorywiseimportancesampl": 1, "seq_len": 4, "seq_len_input": 4, "serv": 4, "set": [0, 1, 2, 3, 4], "set_paramet": 2, "sever": [0, 2, 3], "shape": 4, "shi": 3, "short": 1, "short_nam": 1, "should": [1, 2, 3], "sigma": 2, "signific": 2, "significance_level": 2, "sim": 3, "similar": 2, "simpl": [2, 4], "sinc": [3, 4], "singh": 3, "slightli": 3, "slope": 4, "small": [1, 3], "snipw": [1, 2], "snpdi": 1, "snti": 1, "some": 4, "someth": 2, "sound": 3, "sourc": 0, "space": [1, 4], "specif": 1, "sqrt": 2, "stabl": 3, "stand": [0, 3], "standard": 2, "state": [1, 3, 4], "state_dim": 4, "state_in_0": 4, "state_input": 4, "state_output_nam": 4, "static": 2, "statist": [1, 4], "std": 2, "step": [3, 4], "steps_per_episod": 1, "still": 3, "stochast": 3, "str": [1, 2, 4], "strang": 3, "strategi": 1, "structur": 4, "student": 2, "studi": 3, "subclass": [1, 4], "subset": 4, "suffer": 1, "suit": 0, "sum": 3, "sum_": [1, 3], "supervis": 1, "support": [0, 2, 4], "sutton": 3, "swaminathan": 3, "synthet": 2, "system": 0, "t": [1, 2, 3], "t_": 2, "take": 3, "taken": 1, "target": [0, 1, 2, 3], "target_polici": 2, "target_policy_1": 2, "target_policy_2": 2, "target_policy_3": 2, "target_policy_action_prob": 2, "tau": 3, "techniqu": [0, 1], "temperatur": 4, "test": 2, "thei": 4, "them": 3, "thi": [1, 2, 3, 4], "threshold": 4, "ti": 1, "time": [1, 3, 4], "tool": 0, "top": 2, "top_k": 2, "top_k_result": 2, "toward": 3, "trace": 3, "train": [1, 3, 4], "train_act": 4, "train_ob": 4, "trajectori": [1, 3], "trajectoryperdecisionmixin": 1, "trajectorywiseimportancesampl": 1, "transform": 4, "transit": 3, "true": [2, 3], "tupl": 4, "twi": 3, "two": 3, "txt": 0, "type": 1, "typic": 1, "uehara": 3, "umass": 1, "unbias": 3, "under": [1, 3, 4], "underli": 4, "uniformli": 4, "unknown": 3, "up": 3, "updat": 4, "upper": 2, "upper_bound": 2, "uppercas": 1, "us": [0, 1, 2, 3, 4], "usag": [2, 4], "v_": [1, 3], "vac": 0, "valid": 1, "valu": [1, 2, 3, 4], "valuat": 0, "variabl": 3, "varianc": [1, 3], "variant": 3, "veri": 3, "version": [0, 3], "viewcont": 1, "visual": [0, 4], "voloshin": 3, "w": 1, "w_": 1, "wai": 3, "wang": 3, "want": [0, 3], "we": [3, 4], "weight": [1, 2, 3], "when": [1, 2, 3], "where": [0, 1, 2, 4], "whether": 4, "which": [0, 3, 4], "wise": [1, 3], "with_epsilon": 4, "with_nam": [2, 4], "without": 3, "word": 3, "world": 3, "would": [3, 4], "wrap": 3, "x": [1, 2, 3], "y": 3, "you": [0, 1, 3, 4], "your": 4, "yue": 3, "z": 3, "zero": [3, 4]}, "titles": ["Welcome to Hopes!", "Hopes: Estimators", "Hopes: Selection", "Hopes: finding the best policy", "Hopes: Policies"], "titleterms": {"": [0, 3], "assumpt": 3, "best": 3, "box": 0, "detail": 2, "document": [1, 4], "doe": 3, "estim": 1, "evalu": 3, "find": 3, "hope": [0, 1, 2, 3, 4], "how": 3, "implement": [1, 2, 4], "import": 3, "instal": 0, "introduct": 2, "method": 3, "new": [1, 4], "off": 3, "other": 3, "polici": [3, 4], "refer": 3, "regular": 3, "roadmap": [1, 2], "sampl": 3, "select": 2, "what": [0, 3], "why": 0, "work": 3}})