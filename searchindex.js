Search.setIndex({"docnames": ["index", "ope/index", "ops/index", "overview/index", "policy/index"], "filenames": ["index.rst", "ope/index.rst", "ops/index.rst", "overview/index.rst", "policy/index.rst"], "titles": ["Welcome to Hopes!", "Hopes: Estimators", "Hopes: Selection", "Overview of Hopes package", "Hopes: Policies"], "terms": {"welcom": 0, "hope": 0, "x": 1, "invers": 1, "probabl": [1, 4], "weight": 1, "ipw": [1, 2], "self": 1, "normal": 1, "snipw": [1, 2], "direct": 1, "method": [1, 4], "dm": 1, "doubli": 1, "robust": 1, "dr": 1, "trajectori": 1, "wise": 1, "import": 1, "sampl": 1, "twi": 1, "per": 1, "decis": 1, "pdi": 1, "current": [1, 4], "follow": [1, 4], "ar": [1, 3, 4], "class": [1, 4], "op": [1, 2], "inverseprobabilityweight": [1, 2], "base": [1, 2, 3, 4], "baseestim": [1, 2], "v_": 1, "pi_e": 1, "d": 1, "frac": 1, "1": [1, 2, 4], "n": 1, "sum_": 1, "t": 1, "p": 1, "s_t": 1, "a_t": 1, "r_t": 1, "where": [1, 3, 4], "i": [1, 2, 3, 4], "offlin": [1, 3, 4], "collect": [1, 4], "dataset": [1, 2, 3, 4], "defin": [1, 4], "pi_b": 1, "target": [1, 2, 3], "polici": [1, 2, 3], "behavior": [1, 2], "reward": [1, 2, 4], "observ": [1, 4], "time": [1, 4], "number": [1, 2], "thi": [1, 2, 4], "ha": 1, "gener": 1, "high": 1, "varianc": 1, "especi": 1, "small": 1, "can": [1, 2, 4], "improv": 1, "us": [1, 2, 3, 4], "refer": 1, "http": [1, 3], "scholarwork": 1, "umass": 1, "edu": 1, "cgi": 1, "viewcont": 1, "articl": 1, "1079": 1, "context": [1, 3], "cs_faculty_pub": 1, "estimate_policy_valu": 1, "float": [1, 2, 4], "valu": [1, 2, 4], "estimate_weighted_reward": 1, "ndarrai": [1, 2, 4], "selfnormalizedinverseprobabilityweight": [1, 2], "paper": 1, "nip": 1, "cc": 1, "paper_fil": 1, "2015": 1, "hash": 1, "39027dfad5138c9ca0c474d71db915c3": 1, "abstract": [1, 4], "html": 1, "directmethod": 1, "q_model_cl": 1, "type": 1, "regressionbasedrewardmodel": 1, "behavior_policy_ob": 1, "behavior_policy_act": 1, "behavior_policy_reward": 1, "steps_per_episod": 1, "int": [1, 2, 4], "q_model_typ": 1, "str": [1, 2, 4], "random_forest": [1, 2, 4], "q_model_param": 1, "dict": [1, 2, 4], "none": [1, 4], "q": 1, "A": [1, 4], "s_0": 1, "model": [1, 4], "train": [1, 4], "expect": [1, 2], "initi": [1, 2], "state": [1, 4], "under": [1, 4], "action": [1, 4], "taken": 1, "set": [1, 2, 3, 4], "supervis": 1, "learn": 1, "The": [1, 2, 4], "perform": [1, 3, 4], "depend": [1, 4], "qualiti": 1, "check_paramet": 1, "check": 1, "paramet": [1, 2, 4], "valid": 1, "plu": 1, "addit": 1, "fit": [1, 2, 4], "return": [1, 2, 4], "statist": [1, 4], "To": [1, 4], "you": [1, 4], "need": [1, 4], "subclass": [1, 4], "It": [1, 3, 4], "should": [1, 2], "typic": 1, "below": 1, "abc": [1, 4], "all": [1, 4], "call": 1, "befor": 1, "overridden": [1, 4], "add": 1, "have": [1, 4], "sever": 1, "assumpt": [1, 4], "must": [1, 4], "met": 1, "coverag": 1, "non": 1, "zero": [1, 4], "take": 1, "100": 1, "necessari": [1, 4], "e": 1, "both": [1, 4], "an": [1, 2, 3, 4], "some": [1, 4], "we": [1, 4], "enforc": 1, "here": 1, "avoid": 1, "numer": 1, "issu": 1, "posit": 1, "neg": 1, "abl": 1, "get": 1, "lower": [1, 2], "bound": [1, 2, 4], "specif": 1, "comput": [1, 4], "estimate_policy_value_with_confidence_interv": 1, "num_sampl": 1, "1000": 1, "significance_level": [1, 2], "0": [1, 2, 4], "05": [1, 2], "confid": [1, 2], "interv": [1, 2], "bootstrap": 1, "input": [1, 4], "data": [1, 4], "from": [1, 3, 4], "exampl": [1, 2, 4], "set_paramet": 1, "target_policy_action_prob": 1, "behavior_policy_action_prob": 1, "metric": [1, 2], "print": [1, 2], "output": [1, 2, 4], "lower_bound": [1, 2], "2": 1, "upper_bound": [1, 2], "4": 1, "mean": [1, 2], "3": [1, 3, 4], "std": [1, 2], "signific": 1, "level": 1, "dictionari": 1, "contain": 1, "kei": 1, "given": [1, 2, 4], "upper": 1, "standard": 1, "deviat": 1, "receiv": 1, "short_nam": 1, "short": 1, "name": [1, 4], "custom": 1, "By": [1, 4], "default": [1, 4], "abbrevi": 1, "ie": 1, "run": 2, "off": [2, 3, 4], "evalu": [2, 3, 4], "experi": 2, "best": 2, "simpl": [2, 4], "synthet": 2, "random": [2, 4], "creat": 2, "behavior_polici": 2, "classificationbasedpolici": [2, 4], "ob": [2, 4], "act": [2, 4], "classification_model": [2, 4], "logist": [2, 4], "target_policy_1": 2, "randompolici": [2, 4], "num_act": [2, 4], "with_nam": [2, 4], "p1": 2, "target_policy_2": 2, "p2": 2, "target_policy_3": 2, "p3": 2, "estim": [2, 3, 4], "offpolicyevalu": 2, "rew": 2, "fail_fast": 2, "true": 2, "result": 2, "target_polici": 2, "top": 2, "k": 2, "90": 2, "top_k_result": 2, "offpolicyselect": 2, "select_top_k": 2, "produc": 2, "similar": 2, "510251": 2, "00788465": 2, "497324": 2, "522907": 2, "499158": 2, "00523288": 2, "490235": 2, "507513": 2, "list": [2, 4], "bool": 2, "object": 2, "usag": [2, 4], "collected_ob": 2, "collected_act": 2, "as_datafram": 2, "509084": 2, "00802636": 2, "496008": 2, "522423": 2, "499934": 2, "00515545": 2, "491553": 2, "508592": 2, "offpolicyevaluationresult": 2, "instanc": [2, 4], "one": 2, "each": [2, 4], "static": 2, "evaluation_result": 2, "top_k": 2, "multipl": 2, "hvac": 3, "select": [3, 4], "python": 3, "rl": 3, "control": [3, 4], "offer": 3, "tool": 3, "compar": 3, "baselin": [3, 4], "character": 3, "log": [3, 4], "techniqu": 3, "particularli": 3, "suit": 3, "rule": [3, 4], "support": [3, 4], "version": 3, "10": [3, 4], "pypi": 3, "pip": 3, "sourc": 3, "develop": 3, "git": 3, "clone": 3, "github": 3, "com": 3, "airboxlab": 3, "cd": 3, "poetri": 3, "r": 3, "requir": 3, "txt": 3, "thei": 4, "aim": 4, "provid": 4, "integr": 4, "actual": 4, "which": 4, "model_param": 4, "classif": 4, "predict": 4, "likelihood": 4, "In": 4, "absenc": 4, "pair": 4, "would": 4, "been": 4, "regress": 4, "forest": 4, "mlp": 4, "reg_polici": 4, "train_ob": 4, "train_act": 4, "fit_stat": 4, "act_prob": 4, "compute_action_prob": 4, "new_ob": 4, "accuraci": 4, "f1": 4, "score": 4, "log_likelihood": 4, "piecewiselinearpolici": 4, "num_seg": 4, "actions_bin": 4, "piecewis": 4, "linear": 4, "segment": 4, "threshold": 4, "slope": 4, "distribut": 4, "over": 4, "drawn": 4, "bm": 4, "reset": 4, "outdoor": 4, "air": 4, "function": 4, "temperatur": 4, "minimum": 4, "maximum": 4, "axi": 4, "also": 4, "help": 4, "schedul": 4, "sinc": 4, "determinist": 4, "assum": 4, "assign": 4, "almost": 4, "other": 4, "being": 4, "continu": 4, "discret": 4, "space": 4, "done": 4, "bin": 4, "nearest": 4, "rmse": 4, "r\u00b2": 4, "r2": 4, "onnxmodelbasedpolici": 4, "onnx_model_path": 4, "path": 4, "obs_input": 4, "tupl": 4, "dtype": 4, "state_dim": 4, "seq_len": 4, "prev_n_act": 4, "prev_n_reward": 4, "state_input": 4, "seq_len_input": 4, "prev_actions_input": 4, "prev_rewards_input": 4, "state_output_nam": 4, "action_output_nam": 4, "action_probs_output_nam": 4, "action_log_probs_output_nam": 4, "action_dist_inputs_output_nam": 4, "exist": 4, "onnx": 4, "make": 4, "opinion": 4, "about": 4, "structur": 4, "mai": 4, "overrid": 4, "your": 4, "doe": 4, "attent": 4, "mechan": 4, "updat": 4, "step": 4, "option": 4, "layer": 4, "underli": 4, "pre": 4, "rai": 4, "rllib": 4, "save": 4, "algorithm": 4, "export_policy_model": 4, "transform": 4, "pass": 4, "previou": 4, "onnx_file_path": 4, "default_polici": 4, "np": 4, "float32": 4, "32": 4, "num_transform": 4, "memori": 4, "attention_dim": 4, "state_in_0": 4, "int32": 4, "prev_act": 4, "int64": 4, "reshape_5": 4, "cond_1": 4, "merg": 4, "model_2": 4, "dense_6": 4, "biasadd": 4, "rand": 4, "15": 4, "compute_reward": 4, "onli": 4, "shape": 4, "batch_siz": 4, "obs_dim": 4, "map_input": 4, "prepar": 4, "properti": 4, "output_nam": 4, "subset": 4, "reset_st": 4, "functionbasedpolici": 4, "policy_funct": 4, "callabl": 4, "map": 4, "uniformli": 4, "serv": 4, "comparison": 4, "select_act": 4, "fals": 4, "": 4, "whether": 4, "with_epsilon": 4, "epsilon": 4, "greedi": 4}, "objects": {"hopes.ope.estimators": [[1, 0, 1, "", "BaseEstimator"], [1, 0, 1, "", "DirectMethod"], [1, 0, 1, "", "InverseProbabilityWeighting"], [1, 0, 1, "", "SelfNormalizedInverseProbabilityWeighting"]], "hopes.ope.estimators.BaseEstimator": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_policy_value_with_confidence_interval"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "set_parameters"], [1, 1, 1, "", "short_name"]], "hopes.ope.estimators.DirectMethod": [[1, 1, 1, "", "check_parameters"], [1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"], [1, 1, 1, "", "fit"]], "hopes.ope.estimators.InverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting": [[1, 1, 1, "", "estimate_policy_value"], [1, 1, 1, "", "estimate_weighted_rewards"]], "hopes.ope.evaluation": [[2, 0, 1, "", "OffPolicyEvaluation"]], "hopes.ope.evaluation.OffPolicyEvaluation": [[2, 1, 1, "", "evaluate"]], "hopes.ope.selection": [[2, 0, 1, "", "OffPolicySelection"]], "hopes.ope.selection.OffPolicySelection": [[2, 1, 1, "", "select_top_k"]], "hopes.policy": [[4, 0, 1, "", "ClassificationBasedPolicy"], [4, 0, 1, "", "FunctionBasedPolicy"], [4, 0, 1, "", "OnnxModelBasedPolicy"], [4, 0, 1, "", "PiecewiseLinearPolicy"], [4, 0, 1, "", "Policy"], [4, 0, 1, "", "RandomPolicy"]], "hopes.policy.ClassificationBasedPolicy": [[4, 1, 1, "", "fit"], [4, 1, 1, "", "log_likelihoods"]], "hopes.policy.FunctionBasedPolicy": [[4, 1, 1, "", "log_likelihoods"]], "hopes.policy.OnnxModelBasedPolicy": [[4, 1, 1, "", "compute_reward"], [4, 1, 1, "", "log_likelihoods"], [4, 1, 1, "", "map_inputs"], [4, 2, 1, "", "output_names"], [4, 1, 1, "", "reset_state"]], "hopes.policy.PiecewiseLinearPolicy": [[4, 1, 1, "", "fit"], [4, 1, 1, "", "log_likelihoods"]], "hopes.policy.Policy": [[4, 1, 1, "", "compute_action_probs"], [4, 1, 1, "", "log_likelihoods"], [4, 2, 1, "", "name"], [4, 1, 1, "", "select_action"], [4, 1, 1, "", "with_epsilon"], [4, 1, 1, "", "with_name"]], "hopes.policy.RandomPolicy": [[4, 1, 1, "", "log_likelihoods"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"]}, "titleterms": {"hope": [1, 2, 3, 4], "estim": 1, "roadmap": 1, "implement": [1, 4], "document": [1, 2, 4], "new": [1, 4], "select": 2, "class": 2, "overview": 3, "packag": 3, "what": 3, "": 3, "box": 3, "instal": 3, "polici": 4}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 60}, "alltitles": {"Hopes: Estimators": [[1, "hopes-estimators"]], "Roadmap": [[1, "roadmap"]], "Implemented estimators": [[1, "implemented-estimators"]], "Estimators documentation": [[1, "estimators-documentation"]], "Implementing a new estimator": [[1, "implementing-a-new-estimator"]], "Hopes: Selection": [[2, "hopes-selection"]], "Classes documentation": [[2, "classes-documentation"]], "Overview of Hopes package": [[3, "overview-of-hopes-package"]], "What\u2019s in the box?": [[3, "what-s-in-the-box"]], "Installation": [[3, "installation"]], "Hopes: Policies": [[4, "hopes-policies"]], "Implemented policies": [[4, "implemented-policies"]], "Policies documentation": [[4, "policies-documentation"]], "Implementing a new policy": [[4, "implementing-a-new-policy"]]}, "indexentries": {"baseestimator (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.BaseEstimator"]], "directmethod (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.DirectMethod"]], "inverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting"]], "selfnormalizedinverseprobabilityweighting (class in hopes.ope.estimators)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting"]], "check_parameters() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.check_parameters"]], "check_parameters() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.check_parameters"]], "estimate_policy_value() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_policy_value"]], "estimate_policy_value() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_policy_value"]], "estimate_policy_value_with_confidence_interval() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_policy_value_with_confidence_interval"]], "estimate_weighted_rewards() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.inverseprobabilityweighting method)": [[1, "hopes.ope.estimators.InverseProbabilityWeighting.estimate_weighted_rewards"]], "estimate_weighted_rewards() (hopes.ope.estimators.selfnormalizedinverseprobabilityweighting method)": [[1, "hopes.ope.estimators.SelfNormalizedInverseProbabilityWeighting.estimate_weighted_rewards"]], "fit() (hopes.ope.estimators.directmethod method)": [[1, "hopes.ope.estimators.DirectMethod.fit"]], "set_parameters() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.set_parameters"]], "short_name() (hopes.ope.estimators.baseestimator method)": [[1, "hopes.ope.estimators.BaseEstimator.short_name"]], "offpolicyevaluation (class in hopes.ope.evaluation)": [[2, "hopes.ope.evaluation.OffPolicyEvaluation"]], "offpolicyselection (class in hopes.ope.selection)": [[2, "hopes.ope.selection.OffPolicySelection"]], "evaluate() (hopes.ope.evaluation.offpolicyevaluation method)": [[2, "hopes.ope.evaluation.OffPolicyEvaluation.evaluate"]], "select_top_k() (hopes.ope.selection.offpolicyselection static method)": [[2, "hopes.ope.selection.OffPolicySelection.select_top_k"]], "classificationbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.ClassificationBasedPolicy"]], "functionbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.FunctionBasedPolicy"]], "onnxmodelbasedpolicy (class in hopes.policy)": [[4, "hopes.policy.OnnxModelBasedPolicy"]], "piecewiselinearpolicy (class in hopes.policy)": [[4, "hopes.policy.PiecewiseLinearPolicy"]], "policy (class in hopes.policy)": [[4, "hopes.policy.Policy"]], "randompolicy (class in hopes.policy)": [[4, "hopes.policy.RandomPolicy"]], "compute_action_probs() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.compute_action_probs"]], "compute_reward() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.compute_reward"]], "fit() (hopes.policy.classificationbasedpolicy method)": [[4, "hopes.policy.ClassificationBasedPolicy.fit"]], "fit() (hopes.policy.piecewiselinearpolicy method)": [[4, "hopes.policy.PiecewiseLinearPolicy.fit"]], "log_likelihoods() (hopes.policy.classificationbasedpolicy method)": [[4, "hopes.policy.ClassificationBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.functionbasedpolicy method)": [[4, "hopes.policy.FunctionBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.piecewiselinearpolicy method)": [[4, "hopes.policy.PiecewiseLinearPolicy.log_likelihoods"]], "log_likelihoods() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.log_likelihoods"]], "log_likelihoods() (hopes.policy.randompolicy method)": [[4, "hopes.policy.RandomPolicy.log_likelihoods"]], "map_inputs() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.map_inputs"]], "name (hopes.policy.policy property)": [[4, "hopes.policy.Policy.name"]], "output_names (hopes.policy.onnxmodelbasedpolicy property)": [[4, "hopes.policy.OnnxModelBasedPolicy.output_names"]], "reset_state() (hopes.policy.onnxmodelbasedpolicy method)": [[4, "hopes.policy.OnnxModelBasedPolicy.reset_state"]], "select_action() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.select_action"]], "with_epsilon() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.with_epsilon"]], "with_name() (hopes.policy.policy method)": [[4, "hopes.policy.Policy.with_name"]]}})